{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# Copyright (c) Meta Platforms, Inc. and affiliates.\n",
    "#\n",
    "# This source code is licensed under the MIT license found in the\n",
    "# LICENSE file in the root directory of this source tree.\n",
    "import argparse\n",
    "import dataclasses\n",
    "import sys\n",
    "import timeit\n",
    "from typing import Tuple\n",
    "import pickle\n",
    "import click\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import open3d\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# from home_robot.mapping.voxel import SparseVoxelMap\n",
    "from home_robot.utils.point_cloud_torch import unproject_masked_depth_to_xyz_coordinates\n",
    "\n",
    "from pytorch3d.vis.plotly_vis import AxisArgs, plot_scene, get_camera_wireframe\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from home_robot.datasets.eqa.dataset import EQADataset\n",
    "config_dict = dict(\n",
    "    dataset_name = 'eqa',\n",
    "    camera_params = dict(\n",
    "        image_height=1080,\n",
    "        image_width=1920,\n",
    "        # png_depth_scale = 1000.0 #for depth image in png format\n",
    "        png_depth_scale = 6553.5 #for depth image in png format\n",
    "    )\n",
    ")\n",
    "\n",
    "dataset = EQADataset(\n",
    "        config_dict,\n",
    "        # '/checkpoint/maksymets/eaif/datasets/eqa-v2/frames/scannet-v0/',\n",
    "        # '108-scannet-scene0354_00',\n",
    "        '/checkpoint/maksymets/eaif/datasets/eqa-v2/frames/hm3d-v0/',\n",
    "        '000-hm3d-BFRyYbPCCPE',\n",
    "        desired_height=480,\n",
    "        desired_width=853,\n",
    "        stride = 1,\n",
    "        device='cpu'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short_dataset = [dataset[i] for i in range(3)]\n",
    "short_dataset = [v for v in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb, depth, Ks, pose = zip(*short_dataset)\n",
    "rgb, depth, Ks, pose = [torch.stack(v) for v in [rgb, depth, Ks, pose]]\n",
    "print(len(dataset))\n",
    "plt.imshow(rgb[0].cpu() / 255.)\n",
    "plt.show()\n",
    "plt.imshow(rgb[1].cpu() / 255.)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.tensor([[1., 0, 0, 0], [0, -1, 0, 0], [0, 0, -1, 0], [0, 0, 0, 1]])\n",
    "v.inverse() - v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.structures import Pointclouds\n",
    "from pytorch3d.vis.plotly_vis import AxisArgs, plot_scene, get_camera_wireframe\n",
    "from home_robot.utils.bboxes_3d_plotly import plot_scene_with_bboxes\n",
    "\n",
    "unprojected = unproject_masked_depth_to_xyz_coordinates(\n",
    "    # depth = depth[0, None].unsqueeze(1),\n",
    "    # pose = poses_opencv[0, None],\n",
    "    depth = depth.unsqueeze(1).squeeze(-1),\n",
    "    pose = pose,\n",
    "    inv_intrinsics = torch.linalg.inv(Ks)[:, :3, :3],\n",
    "    # mask: Optional[torch.Tensor] = None,\n",
    ") \n",
    "\n",
    "ptc = Pointclouds(\n",
    "    [unprojected.reshape(-1, 3)],\n",
    "    features = [rgb.reshape(-1,3) / 255.],\n",
    ").subsample(100000)\n",
    "\n",
    "fig = plot_scene({\n",
    "    \"global scene\": dict(\n",
    "        ptc=ptc\n",
    "    )\n",
    "    },\n",
    "    xaxis={\"backgroundcolor\":\"rgb(200, 200, 230)\"},\n",
    "    yaxis={\"backgroundcolor\":\"rgb(230, 200, 200)\"},\n",
    "    zaxis={\"backgroundcolor\":\"rgb(200, 230, 200)\"}, \n",
    "    axis_args=AxisArgs(showgrid=True),\n",
    "    pointcloud_marker_size=3,\n",
    "    pointcloud_max_points=200_000,\n",
    "    height=1000,\n",
    "    # width=1000,\n",
    ")\n",
    "\n",
    "add_camera_poses(fig, pose)\n",
    "fig.update_layout(\n",
    "    # width=width,\n",
    "    height=1000,\n",
    "    # aspectmode=\"data\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.rawpose[1]\n",
    "pose[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
