{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import h5py\n",
    "import torch\n",
    "\n",
    "from conceptfusion import ConceptFusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "from hydra import initialize, compose\n",
    "import hydra\n",
    "\n",
    "hydra.core.global_hydra.GlobalHydra.instance().clear()\n",
    "initialize('configs')\n",
    "\n",
    "args = compose(config_name='conceptfusion')\n",
    "\n",
    "episode_file = \"/srv/flash1/kyadav32/datasets/ovmm/hm3d_512x512/0001.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_grad_enabled(False)\n",
    "\n",
    "concept_fusion = ConceptFusion(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "concept_fusion.clear()\n",
    "\n",
    "with open(\"/srv/flash1/kyadav32/datasets/ovmm/scannet/scannet_dataset.pkl\", \"rb\") as f:\n",
    "    dataset = pickle.load(f)\n",
    "\n",
    "with open(\"/srv/flash1/kyadav32/datasets/ovmm/scannet/scannet_scene0011_00_obs.pkl\", \"rb\") as f:\n",
    "    dataset_0 = [pickle.load(f)]\n",
    "\n",
    "# SETUP EVAL\n",
    "class_id_to_class_names = dict(\n",
    "    zip(\n",
    "        dataset.METAINFO[\"CLASS_IDS\"],  # IDs [1, 3, 4, 5, ..., 65]\n",
    "        dataset.METAINFO[\"CLASS_NAMES\"],  # [wall, floor, cabinet, ...]\n",
    "    )\n",
    ")\n",
    "# If this is an open-vocab detector, they sometimes require a vocab\n",
    "concept_fusion.set_vocabulary(class_id_to_class_names)\n",
    "\n",
    "keys = [\n",
    "    \"images\",\n",
    "    \"depths\",\n",
    "    \"poses\",\n",
    "    \"intrinsics\",\n",
    "    \"boxes_aligned\",\n",
    "    \"box_classes\",\n",
    "]\n",
    "\n",
    "concept_fusion.dbscan_params.epsilon = 0.3\n",
    "concept_fusion.dbscan_params.min_samples = 25\n",
    "concept_fusion.similarity_params.similarity_thresh = 0.9\n",
    "\n",
    "gt_bounds, gt_classes, pred_bounds, pred_classes, pred_scores = [], [], [], [], []\n",
    "for scene_obs in tqdm(dataset_0, desc=\"Evaluating scenes...\"):\n",
    "    # Move to device\n",
    "    for k in keys:\n",
    "        scene_obs[k] = scene_obs[k].to(concept_fusion.device)\n",
    "\n",
    "    # Eval each scene and move to CPU\n",
    "    queries = {\n",
    "        int(clas): class_id_to_class_names[int(clas)]\n",
    "        for clas in scene_obs[\"box_classes\"].unique()\n",
    "    }\n",
    "    instances_dict = concept_fusion.build_scene_and_get_instances_for_queries(\n",
    "        scene_obs, queries.values()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_fusion.show_point_cloud_pytorch3d(instances_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "\n",
    "pcd = o3d.io.read_point_cloud(\"/srv/flash1/kyadav32/datasets/ovmm/scannet/scannet_scene0011_00_gt.ply\")\n",
    "\n",
    "from pytorch3d.structures import Pointclouds\n",
    "from pytorch3d.vis.plotly_vis import AxisArgs\n",
    "from home_robot.utils.bboxes_3d import BBoxes3D\n",
    "\n",
    "from home_robot.utils.bboxes_3d_plotly import plot_scene_with_bboxes\n",
    "from home_robot.utils.data_tools.dict import update\n",
    "\n",
    "from utils import COLOR_LIST\n",
    "\n",
    "traces = {}\n",
    "\n",
    "pc_xyz, pc_rgb = pcd.points, pcd.colors\n",
    "pc_xyz = torch.tensor(np.asarray(pc_xyz), device=\"cuda\")\n",
    "pc_rgb = torch.tensor(np.asarray(pc_rgb), device=\"cuda\")\n",
    "\n",
    "traces[\"Points\"] = Pointclouds(points=[pc_xyz], features=[pc_rgb*255])\n",
    "\n",
    "\n",
    "box_classes = dataset_0[0]['box_classes']\n",
    "box_bounds = dataset_0[0]['boxes_aligned']\n",
    "\n",
    "bounds, names, colors = {}, {}, {}\n",
    "for class_id, bound in zip(box_classes, box_bounds):\n",
    "    \n",
    "    class_name = class_id_to_class_names[class_id.item()]\n",
    "\n",
    "    if class_name not in bounds:\n",
    "        bounds[class_name] = []\n",
    "        names[class_name] = []\n",
    "        colors[class_name] = []\n",
    "    bounds[class_name].append(bound)\n",
    "    names[class_name].append(torch.tensor(class_id, device='cuda'))\n",
    "    colors[class_name].append(torch.tensor(COLOR_LIST[class_id % len(COLOR_LIST)], device='cuda'))\n",
    "for class_name in box_classes.keys():\n",
    "    detected_boxes = BBoxes3D(\n",
    "        bounds=[torch.stack(bounds, dim=0)],\n",
    "        features=[torch.stack(colors, dim=0)],\n",
    "        names=[torch.stack(names, dim=0).unsqueeze(-1)],\n",
    "    )\n",
    "    traces[class_name + \"_bbox\"] = detected_boxes\n",
    "\n",
    "\n",
    "_default_plot_args = dict(\n",
    "    xaxis={\"backgroundcolor\": \"rgb(200, 200, 230)\"},\n",
    "    yaxis={\"backgroundcolor\": \"rgb(230, 200, 200)\"},\n",
    "    zaxis={\"backgroundcolor\": \"rgb(200, 230, 200)\"},\n",
    "    axis_args=AxisArgs(showgrid=True),\n",
    "    pointcloud_marker_size=3,\n",
    "    pointcloud_max_points=500_000,\n",
    "    boxes_plot_together=False,\n",
    "    boxes_wireframe_width=3,\n",
    ")\n",
    "fig = plot_scene_with_bboxes(\n",
    "    plots={f\"Conceptfusion Pointcloud\": traces},\n",
    "    **update(_default_plot_args, {}),\n",
    ")\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    width=1600,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_fusion.similarity_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from plyfile import PlyData, PlyElement\n",
    "\n",
    "with open(\"/srv/flash1/kyadav32/datasets/ovmm/scannet/scannet_scene0011_00_gt.ply\", \"rb\") as f:\n",
    "    # load ply file\n",
    "    plydata = PlyData.read(f)\n",
    "\n",
    "# Print element names and properties\n",
    "for element in plydata.elements:\n",
    "    print(f\"Element name: {element.name}\")\n",
    "    for prop in element.properties:\n",
    "        print(f\"\\tProperty name: {prop.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_fusion.dbscan_params.epsilon = 0.3\n",
    "concept_fusion.dbscan_params.min_samples = 25\n",
    "concept_fusion.similarity_params.viz_type = \"thresh\"\n",
    "concept_fusion.similarity_params.similarity_thresh = 0.9\n",
    "\n",
    "instances_dict = concept_fusion.get_instances_for_queries([\"sofa\", \"windows\", \"table\", \"chair\", \"dressing_table\"])\n",
    "concept_fusion._show_point_cloud_pytorch3d(instances_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
